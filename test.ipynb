{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./pickle2/AD00203_1.pickle\", \"rb\") as f:  \n",
    "    dataframe = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./pickle2/AD00204_1.pickle\", \"rb\") as f:  \n",
    "    data2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./pickle/AD00203_3.pickle\", \"rb\") as f:  \n",
    "    dataframe3 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.814"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe[\"results\"][\"knn\"][\"full\"][0][\"f1_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.815"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe3[\"results\"][\"knn\"][\"full\"][0][\"f1_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_scores(dataframe, algorithm=\"knn\"):\n",
    "    f1s = {}\n",
    "    accs = {}\n",
    "    for key in dataframe['results'][algorithm]:\n",
    "        f1a = 0\n",
    "        acc = 0\n",
    "        for i in range(3):\n",
    "            f1a += dataframe['results'][algorithm][key][i]['f1_score']\n",
    "            acc += dataframe['results'][algorithm][key][i]['accuracy']\n",
    "        f1a = round(f1a / 3, 3)\n",
    "        acc = round(acc / 3, 3)\n",
    "        f1s[key] = f1a\n",
    "        accs[key] = acc\n",
    "    return f1s, accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1s_knn, acc_knn = compare_scores(algorithm=\"knn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = []\n",
    "for alg in (\"knn\", \"lrc\", \"svm\", \"mnb\", \"dtc\"):\n",
    "    f1s, acc = compare_scores(data2, alg)\n",
    "    res2.append((alg, f1s[\"full\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2\n",
    "df2 = pd.DataFrame(res2, columns=[\"classifiers\", \"AD00204\"])\n",
    "df2 = df2.set_index(\"classifiers\")\n",
    "df2.index.name = None\n",
    "df2 = df2.T\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(res, columns=[\"classifiers\", \"AD00203\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index(\"classifiers\")\n",
    "df.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_res_df = pd.concat([df, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set (rc = {'figure.figsize':(5, 1)})\n",
    "ax = sns.heatmap(final_res_df, annot=True, fmt='g',cmap=sns.color_palette(\"viridis\", as_cmap=True))\n",
    "# plt.title('Full feature')\n",
    "title = \"Full feature\"\n",
    "ax.set_title(title, fontsize=16, backgroundcolor='gray', color='white', pad=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(f1s.items(), key=lambda item: item[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(acc.items(), key=lambda item: item[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_mtx(algorithm=\"knn\", tryno=1):\n",
    "\n",
    "    truth = dataframe['results'][algorithm][tryno]['ground_truth']\n",
    "    pred = dataframe['results'][algorithm][tryno]['prediction']\n",
    "    cm = confusion_matrix(truth, pred)\n",
    "    return cm\n",
    "    # f = sns.heatmap(cm, annot=True, fmt='d')\n",
    "    # return f\n",
    "\n",
    "def plot_confusion():\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 5), sharey=True)\n",
    "    fig.suptitle('Confusion matrixes for knn with 3 repeats')\n",
    "\n",
    "    # Bulbasaur\n",
    "    sns.heatmap(confusion_mtx(algorithm=\"knn\", tryno=0), ax=axes[0], annot=True, fmt='d')\n",
    "\n",
    "    axes[0].set_title(\"repeat 1\")\n",
    "\n",
    "    # Charmander\n",
    "    sns.heatmap( confusion_mtx(algorithm=\"knn\", tryno=1), ax=axes[1], annot=True, fmt='d')\n",
    "    axes[1].set_title(\"repeat 2\")\n",
    "\n",
    "    # Squirtle\n",
    "    sns.heatmap(confusion_mtx(algorithm=\"knn\", tryno=2), ax=axes[2], annot=True, fmt='d')\n",
    "    axes[2].set_title(\"repeat 3\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
